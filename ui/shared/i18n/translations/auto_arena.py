# -*- coding: utf-8 -*-
"""Auto Arena feature translations for OpenJudge Studio.

Contains translations for:
- Auto Arena feature header and description
- Sidebar settings (judge model, evaluation settings)
- Config panel (task, target models, query settings)
- Progress panel
- Result panel
- History panel
- Report viewer
- Help content

Key prefix conventions:
- arena.*          : General Auto Arena feature
- arena.sidebar.*  : Sidebar settings
- arena.config.*   : Configuration panel
- arena.progress.* : Progress panel
- arena.result.*   : Result panel
- arena.history.*  : History panel
- arena.help.*     : Help content
"""

# =============================================================================
# English Translations
# =============================================================================
EN: dict[str, str] = {
    # Feature Info
    "arena.name": "Auto Arena",
    "arena.description": "Automated model/agent evaluation arena",
    # Tabs
    "arena.tabs.new": "New Evaluation",
    "arena.tabs.history": "History",
    "arena.tabs.help": "Help",
    # Sidebar - Judge Model
    "arena.sidebar.judge_model": "Judge Model",
    "arena.sidebar.judge_provider": "Provider",
    "arena.sidebar.judge_provider_help": "Select API provider for the judge model",
    "arena.sidebar.judge_api_key": "API Key",
    "arena.sidebar.judge_api_key_help": "Your API key for the judge model",
    "arena.sidebar.judge_api_key_configured": "API Key configured",
    "arena.sidebar.judge_api_key_required": "Enter API Key to continue",
    # Sidebar - Evaluation Settings
    "arena.sidebar.eval_settings": "Evaluation Settings",
    "arena.sidebar.queries": "Queries",
    "arena.sidebar.queries_help": "Number of test queries to generate",
    "arena.sidebar.concurrency": "Concurrency",
    "arena.sidebar.concurrency_help": "Maximum concurrent API requests",
    # Sidebar - Output Settings
    "arena.sidebar.output_settings": "Output Settings",
    "arena.sidebar.save_queries": "Save generated queries",
    "arena.sidebar.save_responses": "Save model responses",
    "arena.sidebar.save_details": "Save detailed results",
    "arena.sidebar.generate_report": "Generate evaluation report",
    "arena.sidebar.generate_chart": "Generate win rate chart",
    # Config Panel - Presets
    "arena.config.presets": "Configuration Presets",
    "arena.config.load_preset": "Load Preset",
    "arena.config.save_preset": "Save as Preset",
    "arena.config.preset_name": "Preset Name",
    "arena.config.preset_placeholder": "Enter a name for this preset...",
    "arena.config.save_success": "Preset '{name}' saved successfully!",
    "arena.config.load_success": "Preset '{name}' loaded successfully!",
    "arena.config.delete_success": "Preset '{name}' deleted",
    "arena.config.no_presets": "No presets saved yet",
    # Config Panel - Task
    "arena.config.task": "Task Configuration",
    "arena.config.task_description": "Task Description",
    "arena.config.task_description_placeholder": (
        "Describe the task that the target models/agents will be evaluated on...\n"
        "Example: English to Chinese translation assistant"
    ),
    "arena.config.task_description_help": (
        "A clear description of what the task is about. This helps generate relevant test queries."
    ),
    "arena.config.task_scenario": "Usage Scenario (Optional)",
    "arena.config.task_scenario_placeholder": "Example: Users need to translate articles into Chinese",
    "arena.config.task_scenario_help": "Optional context about when/how the task is used",
    # Config Panel - Target Models
    "arena.config.targets": "Target Models/Agents to Evaluate",
    "arena.config.target_model": "Target",
    "arena.config.add_target": "Add Target",
    "arena.config.remove_target": "Remove this endpoint",
    "arena.config.display_name": "Display Name",
    "arena.config.display_name_help": "Custom display name for reports. Defaults to model name if empty.",
    "arena.config.endpoint_url": "Endpoint URL",
    "arena.config.system_prompt": "System Prompt",
    "arena.config.system_prompt_placeholder": "Optional system prompt for this model/agent...",
    "arena.config.advanced_settings": "Advanced Settings",
    # Config Panel - Query Settings
    "arena.config.query_settings": "Query Generation Settings",
    "arena.config.query_settings_desc": "Configure how test queries are generated",
    "arena.config.seed_queries": "Seed Queries (Optional)",
    "arena.config.seed_queries_placeholder": (
        "Enter example queries, one per line...\nThese help guide the query generation style."
    ),
    "arena.config.temperature": "Temperature",
    "arena.config.temperature_help": "Higher = more diverse queries",
    "arena.config.dedup_threshold": "Dedup Threshold",
    "arena.config.dedup_threshold_help": "Lower = stricter deduplication",
    "arena.config.enable_evolution": "Enable Complexity Evolution (Evol-Instruct)",
    "arena.config.enable_evolution_help": "Progressively increase query complexity",
    # Config Panel - Start Button
    "arena.config.start": "Start Evaluation",
    # Validation
    "arena.validation.task_required": "Task description is required",
    "arena.validation.min_models": "Need 2+ targets with API Key filled ({configured}/{total} configured)",
    "arena.validation.judge_api_required": "Judge model API key is required",
    # Progress Panel
    "arena.progress.title": "Evaluation Progress",
    "arena.progress.empty": "Configure settings and start evaluation to see progress",
    "arena.progress.running": "Running Auto Arena Evaluation...",
    "arena.progress.initializing": "Initializing pipeline...",
    "arena.progress.init_desc": "Initializing Auto Arena Pipeline",
    "arena.progress.task": "Task",
    "arena.progress.target_models": "Target models/agents",
    "arena.progress.queries_to_generate": "Queries to generate",
    "arena.progress.running_pipeline": "Running evaluation pipeline",
    "arena.progress.running_desc": "(this may take several minutes)",
    "arena.progress.pipeline_steps": "The pipeline will:",
    "arena.progress.step1": "Generate test queries",
    "arena.progress.step2": "Collect responses from targets",
    "arena.progress.step3": "Generate evaluation rubrics",
    "arena.progress.step4": "Run pairwise comparisons",
    "arena.progress.step5": "Analyze and rank results",
    "arena.progress.complete": "Evaluation Complete!",
    "arena.progress.completed_success": "Evaluation completed successfully!",
    "arena.progress.best_model": "Best performer",
    "arena.progress.total_queries": "Total queries",
    "arena.progress.total_comparisons": "Total comparisons",
    "arena.progress.rankings": "Rankings",
    "arena.progress.failed": "Evaluation Failed",
    "arena.progress.failed_msg": "Evaluation failed: {error}",
    "arena.progress.resume_tip": "Tip: Check the History tab to resume from the last checkpoint.",
    # Result Panel
    "arena.result.title": "Evaluation Results",
    "arena.result.best_model": "Best Performer",
    "arena.result.win_rate": "Win Rate",
    "arena.result.model_rankings": "Rankings",
    "arena.result.ranking": "Rank",
    "arena.result.model": "Model/Agent",
    "arena.result.summary": "Summary",
    "arena.result.details": "Detailed Results",
    "arena.result.open_folder": "Open Output Folder",
    "arena.result.view_report": "View Report",
    # History Panel
    "arena.history.title": "Evaluation History",
    "arena.history.empty": "No evaluation history yet",
    "arena.history.empty_hint": "Start an evaluation to see it here",
    "arena.history.count": "Showing {count} evaluation(s)",
    "arena.history.view": "View",
    "arena.history.resume": "Resume",
    "arena.history.delete": "Delete",
    "arena.history.task_deleted": "Task {task_id} deleted",
    "arena.history.delete_failed": "Failed to delete task",
    "arena.history.resuming": "Resuming evaluation from checkpoint in {task_id}...",
    "arena.history.resume_complete": "Evaluation resumed and completed!",
    "arena.history.resume_failed": "Failed to resume: {error}",
    # Report Viewer
    "arena.report.back": "Back to History",
    "arena.report.task_not_found": "Task not found: {task_id}",
    "arena.report.no_report": "No report available for this task",
    "arena.report.model_ranking": "Rankings",
    "arena.report.comparison_matrix": "Comparison Matrix",
    "arena.report.generated_queries": "Generated Queries",
    "arena.report.view_all_queries": "View all queries",
    # Help
    "arena.help.title": "Auto Arena Guide",
    "arena.help.step1_title": "Configure Judge Model",
    "arena.help.step1_desc": "Set up the judge model in the sidebar",
    "arena.help.step2_title": "Define Task",
    "arena.help.step2_desc": "Describe the task your models/agents will be evaluated on",
    "arena.help.step3_title": "Add Targets",
    "arena.help.step3_desc": "Configure at least 2 models/agents to compare",
    "arena.help.step4_title": "Start Evaluation",
    "arena.help.step4_desc": 'Click "Start Evaluation" to begin',
    # Preset Panel
    "arena.preset.no_presets": "No presets yet",
    "arena.preset.preset": "Preset",
    "arena.preset.save_preset": "Save Preset",
    "arena.preset.name": "Name",
    "arena.preset.name_help": "Letters, numbers, underscores, hyphens only",
    "arena.preset.save": "Save",
    "arena.preset.name_required": "Name required",
    "arena.preset.cancel": "Cancel",
    "arena.preset.delete_confirm": "Delete {name}? This cannot be undone.",
    "arena.preset.delete": "Delete",
    "arena.preset.presets": "Presets",
    "arena.preset.delete_selected": "Delete selected preset",
    "arena.preset.import_export": "Import / Export",
    "arena.preset.import": "Import",
    "arena.preset.import_failed": "Import failed: {error}",
    "arena.preset.export_yaml": "Export YAML",
    # Report Viewer (continued)
    "arena.report.evaluation_report": "Evaluation Report",
    "arena.report.task_id": "Task ID",
    "arena.report.tab_overview": "Overview",
    "arena.report.tab_report": "Report",
    "arena.report.tab_details": "Details",
    "arena.report.tab_export": "Export",
    "arena.report.test_queries": "Test Queries",
    "arena.report.comparisons": "Comparisons",
    "arena.report.best_model": "Best Performer",
    "arena.report.model_rankings": "Rankings",
    "arena.report.win_rate_chart": "Win Rate Chart",
    "arena.report.report_not_available": "Report not available",
    "arena.report.report_not_generated": "Report not generated for this evaluation",
    "arena.report.no_comparison_details": "No comparison details available",
    "arena.report.comparisons_count": "{count} comparisons (showing unique queries)",
    "arena.report.showing_comparisons": "Showing {shown} of {total} comparisons",
    "arena.report.export_options": "Export Options",
    "arena.report.json_export": "JSON Export",
    "arena.report.csv_export": "CSV Export",
    "arena.report.json_desc": "Full evaluation data including queries, responses, and results",
    "arena.report.csv_desc": "Rankings and win rates in spreadsheet format",
    "arena.report.download_json": "Download JSON",
    "arena.report.download_csv": "Download CSV",
    "arena.report.report_download": "Report Download",
    "arena.report.download_report_md": "Download Report (MD)",
    "arena.report.download_chart_png": "Download Chart (PNG)",
    "arena.report.report_na": "Report N/A",
    "arena.report.chart_na": "Chart N/A",
    "arena.report.winner": "Winner",
    "arena.report.score": "Score",
    "arena.report.tie": "Tie",
    "arena.report.reason": "Reason",
}

# =============================================================================
# Chinese Translations
# =============================================================================
ZH: dict[str, str] = {
    # Feature Info
    "arena.name": "自动竞技场",
    "arena.description": "自动化模型/智能体评测竞技场",
    # Tabs
    "arena.tabs.new": "新建评估",
    "arena.tabs.history": "历史记录",
    "arena.tabs.help": "帮助",
    # Sidebar - Judge Model
    "arena.sidebar.judge_model": "评判模型",
    "arena.sidebar.judge_provider": "服务商",
    "arena.sidebar.judge_provider_help": "选择评判模型的 API 服务商",
    "arena.sidebar.judge_api_key": "API 密钥",
    "arena.sidebar.judge_api_key_help": "评判模型的 API 密钥",
    "arena.sidebar.judge_api_key_configured": "API 密钥已配置",
    "arena.sidebar.judge_api_key_required": "请输入 API 密钥以继续",
    # Sidebar - Evaluation Settings
    "arena.sidebar.eval_settings": "评估设置",
    "arena.sidebar.queries": "查询数",
    "arena.sidebar.queries_help": "要生成的测试查询数量",
    "arena.sidebar.concurrency": "并发数",
    "arena.sidebar.concurrency_help": "最大并发 API 请求数",
    # Sidebar - Output Settings
    "arena.sidebar.output_settings": "输出设置",
    "arena.sidebar.save_queries": "保存生成的查询",
    "arena.sidebar.save_responses": "保存模型响应",
    "arena.sidebar.save_details": "保存详细结果",
    "arena.sidebar.generate_report": "生成评估报告",
    "arena.sidebar.generate_chart": "生成胜率图表",
    # Config Panel - Presets
    "arena.config.presets": "配置预设",
    "arena.config.load_preset": "加载预设",
    "arena.config.save_preset": "保存为预设",
    "arena.config.preset_name": "预设名称",
    "arena.config.preset_placeholder": "输入预设名称...",
    "arena.config.save_success": "预设「{name}」保存成功！",
    "arena.config.load_success": "预设「{name}」加载成功！",
    "arena.config.delete_success": "预设「{name}」已删除",
    "arena.config.no_presets": "暂无保存的预设",
    # Config Panel - Task
    "arena.config.task": "任务配置",
    "arena.config.task_description": "任务描述",
    "arena.config.task_description_placeholder": "描述待评估模型/智能体需要完成的任务...\n示例：英译中翻译助手",
    "arena.config.task_description_help": "清晰描述任务内容，这有助于生成相关的测试查询。",
    "arena.config.task_scenario": "使用场景（可选）",
    "arena.config.task_scenario_placeholder": "示例：用户需要将文章翻译成中文",
    "arena.config.task_scenario_help": "可选的任务使用场景说明",
    # Config Panel - Target Models
    "arena.config.targets": "待评估目标",
    "arena.config.target_model": "目标",
    "arena.config.add_target": "添加目标",
    "arena.config.remove_target": "移除此端点",
    "arena.config.display_name": "显示名称",
    "arena.config.display_name_help": "报告中的自定义显示名称，为空时使用模型名称。",
    "arena.config.endpoint_url": "端点 URL",
    "arena.config.system_prompt": "系统提示词",
    "arena.config.system_prompt_placeholder": "此模型/智能体的可选系统提示词...",
    "arena.config.advanced_settings": "高级设置",
    # Config Panel - Query Settings
    "arena.config.query_settings": "查询生成设置",
    "arena.config.query_settings_desc": "配置测试查询的生成方式",
    "arena.config.seed_queries": "种子查询（可选）",
    "arena.config.seed_queries_placeholder": "输入示例查询，每行一条...\n这些示例有助于引导查询生成的风格。",
    "arena.config.temperature": "温度",
    "arena.config.temperature_help": "越高 = 查询越多样",
    "arena.config.dedup_threshold": "去重阈值",
    "arena.config.dedup_threshold_help": "越低 = 去重越严格",
    "arena.config.enable_evolution": "启用复杂度演进 (Evol-Instruct)",
    "arena.config.enable_evolution_help": "逐步增加查询复杂度",
    # Config Panel - Start Button
    "arena.config.start": "开始评估",
    # Validation
    "arena.validation.task_required": "任务描述为必填项",
    "arena.validation.min_models": "至少需要 2 个填写了 API 密钥的目标（已配置 {configured}/{total}）",
    "arena.validation.judge_api_required": "评判模型 API 密钥为必填项",
    # Progress Panel
    "arena.progress.title": "评估进度",
    "arena.progress.empty": "配置设置并开始评估后查看进度",
    "arena.progress.running": "正在运行自动竞技场评估...",
    "arena.progress.initializing": "正在初始化流水线...",
    "arena.progress.init_desc": "正在初始化自动竞技场流水线",
    "arena.progress.task": "任务",
    "arena.progress.target_models": "目标数",
    "arena.progress.queries_to_generate": "待生成查询数",
    "arena.progress.running_pipeline": "正在运行评估流水线",
    "arena.progress.running_desc": "（可能需要几分钟）",
    "arena.progress.pipeline_steps": "流水线将：",
    "arena.progress.step1": "生成测试查询",
    "arena.progress.step2": "收集目标响应",
    "arena.progress.step3": "生成评估标准",
    "arena.progress.step4": "运行成对比较",
    "arena.progress.step5": "分析并排名结果",
    "arena.progress.complete": "评估完成！",
    "arena.progress.completed_success": "评估成功完成！",
    "arena.progress.best_model": "最佳表现",
    "arena.progress.total_queries": "总查询数",
    "arena.progress.total_comparisons": "总比较数",
    "arena.progress.rankings": "排名",
    "arena.progress.failed": "评估失败",
    "arena.progress.failed_msg": "评估失败：{error}",
    "arena.progress.resume_tip": "提示：前往历史记录标签页从上次检查点继续。",
    # Result Panel
    "arena.result.title": "评估结果",
    "arena.result.best_model": "最佳表现",
    "arena.result.win_rate": "胜率",
    "arena.result.model_rankings": "排名",
    "arena.result.ranking": "排名",
    "arena.result.model": "模型/智能体",
    "arena.result.summary": "摘要",
    "arena.result.details": "详细结果",
    "arena.result.open_folder": "打开输出文件夹",
    "arena.result.view_report": "查看报告",
    # History Panel
    "arena.history.title": "评估历史",
    "arena.history.empty": "暂无评估历史",
    "arena.history.empty_hint": "开始评估后将显示在此处",
    "arena.history.count": "显示 {count} 条记录",
    "arena.history.view": "查看",
    "arena.history.resume": "续传",
    "arena.history.delete": "删除",
    "arena.history.task_deleted": "任务 {task_id} 已删除",
    "arena.history.delete_failed": "删除任务失败",
    "arena.history.resuming": "正在从检查点续传 {task_id}...",
    "arena.history.resume_complete": "评估续传完成！",
    "arena.history.resume_failed": "续传失败：{error}",
    # Report Viewer
    "arena.report.back": "返回历史",
    "arena.report.task_not_found": "未找到任务：{task_id}",
    "arena.report.no_report": "此任务暂无可用报告",
    "arena.report.model_ranking": "排名",
    "arena.report.comparison_matrix": "比较矩阵",
    "arena.report.generated_queries": "生成的查询",
    "arena.report.view_all_queries": "查看所有查询",
    # Help
    "arena.help.title": "自动竞技场指南",
    "arena.help.step1_title": "配置评判模型",
    "arena.help.step1_desc": "在侧边栏设置评判模型",
    "arena.help.step2_title": "定义任务",
    "arena.help.step2_desc": "描述待评估模型/智能体的任务",
    "arena.help.step3_title": "添加目标",
    "arena.help.step3_desc": "配置至少 2 个待比较的模型/智能体",
    "arena.help.step4_title": "开始评估",
    "arena.help.step4_desc": "点击「开始评估」按钮",
    # Preset Panel
    "arena.preset.no_presets": "暂无预设",
    "arena.preset.preset": "预设",
    "arena.preset.save_preset": "保存预设",
    "arena.preset.name": "名称",
    "arena.preset.name_help": "仅支持字母、数字、下划线、连字符",
    "arena.preset.save": "保存",
    "arena.preset.name_required": "名称为必填项",
    "arena.preset.cancel": "取消",
    "arena.preset.delete_confirm": "删除「{name}」？此操作不可撤销。",
    "arena.preset.delete": "删除",
    "arena.preset.presets": "预设",
    "arena.preset.delete_selected": "删除所选预设",
    "arena.preset.import_export": "导入 / 导出",
    "arena.preset.import": "导入",
    "arena.preset.import_failed": "导入失败：{error}",
    "arena.preset.export_yaml": "导出 YAML",
    # Report Viewer (continued)
    "arena.report.evaluation_report": "评估报告",
    "arena.report.task_id": "任务 ID",
    "arena.report.tab_overview": "概览",
    "arena.report.tab_report": "报告",
    "arena.report.tab_details": "详情",
    "arena.report.tab_export": "导出",
    "arena.report.test_queries": "测试查询",
    "arena.report.comparisons": "比较次数",
    "arena.report.best_model": "最佳表现",
    "arena.report.model_rankings": "排名",
    "arena.report.win_rate_chart": "胜率图表",
    "arena.report.report_not_available": "报告不可用",
    "arena.report.report_not_generated": "此评估未生成报告",
    "arena.report.no_comparison_details": "无比较详情",
    "arena.report.comparisons_count": "{count} 次比较（显示唯一查询）",
    "arena.report.showing_comparisons": "显示 {shown} / {total} 次比较",
    "arena.report.export_options": "导出选项",
    "arena.report.json_export": "JSON 导出",
    "arena.report.csv_export": "CSV 导出",
    "arena.report.json_desc": "完整评估数据，包括查询、响应和结果",
    "arena.report.csv_desc": "排名和胜率的表格格式",
    "arena.report.download_json": "下载 JSON",
    "arena.report.download_csv": "下载 CSV",
    "arena.report.report_download": "报告下载",
    "arena.report.download_report_md": "下载报告 (MD)",
    "arena.report.download_chart_png": "下载图表 (PNG)",
    "arena.report.report_na": "报告不可用",
    "arena.report.chart_na": "图表不可用",
    "arena.report.winner": "胜者",
    "arena.report.score": "分数",
    "arena.report.tie": "平局",
    "arena.report.reason": "原因",
}
